
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110963980-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-110963980-1');
	</script>

  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
	/* Color scheme stolen from Sergey Karayev */
	a {
	color: #1772d0;
	text-decoration:none;
	}
	a:focus, a:hover {
	color: #f09228;
	text-decoration:none;
	}
	body,td,th,tr,p,a {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px
	}
	strong {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px;
	}
	heading {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 22px;
	}
	papertitle {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px;
	font-weight: 700
	}
	name {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 32px;
	}
	.one
	{
	width: 160px;
	height: 100px;
	position: relative;
	}
	.two
	{
	width: 160px;
	height: 100px;
	position: absolute;
	transition: opacity .2s ease-in-out;
	-moz-transition: opacity .2s ease-in-out;
	-webkit-transition: opacity .2s ease-in-out;
	}
	.fade {
	 transition: opacity .2s ease-in-out;
	 -moz-transition: opacity .2s ease-in-out;
	 -webkit-transition: opacity .2s ease-in-out;
	}
	span.highlight {
		background-color: #ffffd0;
	}
  </style>
  <link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~barron/seal_icon.png">
  <title>Rafael Rafailov</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
	<tr>
	<td>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td width="67%" valign="middle">
		<p align="center">
		<name>Rafael Rafailov</name><br>
		rafailov at cs dot stanford dot edu
		</p>
		<p>
	  I am a Ph.D. student in <a href="https://cs.stanford.edu/"> Computer Science</a> at <a href="https://www.stanford.edu/">Stanford University</a> advised by 
	  <a href="https://ai.stanford.edu/~cbfinn/">Prof. Chelsea Finn</a> at the 
	  <a href="https://irislab.stanford.edu/">IRIS lab</a>, part of the 
	  <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a>. I am interested in the capability of robots and other agents to develop broadly intelligent behavior through learning and interaction.
	   My reserach is focused on a data-driven approach to embodied AI, which aims to re-use previously collected data to deploy in offline reinforcement, planning and imitation learing, particularly in realistic domains. .
	   I am also interested in model-based learning, generative modelling and real-world deployment of RL.
    </p>
	<p>
	Previously I obtained Masters degrees in 
	  <a href="https://statistics.stanford.edu/"> Statistics </a> and 
	  <a href="https://cs.stanford.edu/"> Computer Science</a> (with distinction in research) also at Stanford. Before that, I graduated from <a href="http://www.berkeley.edu/">UC Berkeley</a> with highest honors in
	  <a href="http://math.berkeley.edu/">Applied Mathematics</a>, <a href="http://statistics.berkeley.edu//">Statistics</a> and <a href="https://www.econ.berkeley.edu///">Economics</a>. Before graduate school I was a junior portfolio manager at Goldman Sachs' <a href="https://www.gsam.com/content/gsam/us/en/advisors/market-insights/gsam-insights/quantinomics.html">Quantitative Investment Startegies (QIS)</a> unit.
		</p>
		<p align=center>

<a href="https://scholar.google.com/citations?user=TwABcRgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
<a href="https://www.semanticscholar.org/author/Rafael-Rafailov/102801230">Semantic Scholar</a> &nbsp/&nbsp
<a href="https://twitter.com/rmrafailov">Twitter</a>
		</p>
		</td>
		<td width="40%">
		<img src="./files/rafael.png" width="250">
		</td>
	  </tr>
	  </table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
            <heading>News</heading>
            <ul>
              <li><strong>January 2022</strong>: Our paper "Vision-Based Manipulators Need to Also See from Their Hands" was elected for an Oral presentation at ICLR.</li>
              <li><strong>September 2021</strong>: Two papers accepted at NeurIPS 2021.</li>
              <li><strong>March 2021</strong>: Our paper Offline Reinforcement Learning from Images with Latent Space Models was selected for an Oral presentation at L4DC.</li>
              <li><strong>March 2021</strong>: I gave a talk the the Intel AI on scaling offline model-based Reinforcement Learning.</li>


            </ul>
        </td></tr>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td width="100%" valign="middle">
		  <heading>Research</heading>
		  <p>
		My research interests lie at the intersection of machine learning, perception, and control for robotics, specifically deep reinforcement learning, imitation learning and meta-learning.
		  </p>
		</td>
	  </tr>
	  </table>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">



    <tr onmouseout="seeing_stop()" onmouseover="seeing_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="seeing_still" style="opacity: 0;"><img src="./files/seeing.png" width=150></div>
                  <img src="./files/seeing.png" width=150 >
              </div>
              <script type="text/javascript">
              function seeing_start() {
                document.getElementById('seeing_still').style.opacity = "1";
              }
              
              function seeing_stop() {
                document.getElementById('seeing_still').style.opacity = "0";
              }
              seeing_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="">
                  <papertitle>Vision-Based Manipulators Need to Also See from Their Hands </papertitle></a><br>
              <a href="https://www.kylehsu.me/">Kyle Hsu*</a>,
              <a href="https://www.linkedin.com/in/moojink/">Moo Jin Kim*</a>,
              <strong>Rafael Rafailov</strong>, 
              <a href="https://jiajunwu.com/">Jiajun Wu</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>International Conference on Learning Representations (ICLR) 2022</em><font color="red"><strong> (Oral, top 1.5%)</strong></font><br>
                <a href="">arXiv</a>
                </p><p></p>
                <p>
                 A hand-centric (eye-in-hand) perspective consistently improves training efficiency and out-of-distribution generalization in robotic manipulation. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts. When hand-centric observability is not sufficient, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck.
                </p>
                </td>
              </tr>
              

    <tr onmouseout="vmail_stop()" onmouseover="vmail_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="vmail_still" style="opacity: 0;"><img src="./files/vmail.png" width=150></div>
                  <img src="./files/vmail.png" width=150 >
              </div>
              <script type="text/javascript">
              function vmail_start() {
                document.getElementById('vmail_still').style.opacity = "1";
              }
     function vmail_stop() {
                document.getElementById('vmail_still').style.opacity = "0";
              }
              vmail_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2107.08829.pdf">
                  <papertitle>Visual Adversarial Imitation Learning using Variational Models</papertitle></a><br>
              <strong>Rafael Rafailov</strong>,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em><br>
                <a href="https://arxiv.org/abs/2107.08829">arXiv</a> / <a href="https://sites.google.com/view/variational-mail/">website</a> / <a href="https://github.com/rmrafailov/VMAIL/">code</a>
                </p><p></p>
                <p>
                We develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. We train a low-dimensional variational dynamics model from high-dimensional image observtions, which provides a strong signal for representation learning. We then use on-policy model-generated data to train an adversarial imitation learning agent which improves sample efficiency and stability of adversarial training. We can also transfer the learned model to new imitation tasks, enabling zero-shot adversarial imitation learning.
                </p>
                </td>
              </tr>
              
    <tr onmouseout="combo_stop()" onmouseover="combo_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="combo_still" style="opacity: 0;"><img src="./files/combo.png" width=150></div>
                  <img src="./files/combo.png" width=150 >
              </div>
              <script type="text/javascript">
              function combo_start() {
                document.getElementById('combo_still').style.opacity = "1";
              }
     function combo_stop() {
                document.getElementById('combo_still').style.opacity = "0";
              }
              combo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2102.08363.pdf">
                  <papertitle>COMBO: Conservative Offline Model-Based Policy Optimization</papertitle></a><br>
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu*</a>,
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar*</a>,
              <strong>Rafael Rafailov</strong>, 
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em><br>
                <a href="https://arxiv.org/abs/2102.08363">arXiv</a>
                </p><p></p>
                <p>
                Model-based offline RL methods rely on explicit uncertainty quantification for incorporating pessimism, which can be difficult and unreliable with complex models. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. We show COMBO with theoretical guarantees and also find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.
                </p>
                </td>
              </tr>


    <tr onmouseout="reflective_explorer_stop()" onmouseover="reflective_explorer_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="reflective_explorer_still" style="opacity: 0;"><img src="./files/reflective_explorer.png" width=150></div>
                  <img src="./files/reflective_explorer.png" width=150 >
              </div>
              <script type="text/javascript">
              function combo_start() {
                document.getElementById('reflective_explorer_still').style.opacity = "1";
              }
              function combo_stop() {
                document.getElementById('reflective_explorer_still').style.opacity = "0";
              }
              reflective_explorer_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://offline-rl-neurips.github.io/2021/pdf/54.pdf">
                  <papertitle>The Reflective Explorer: Online Meta-Exploration from Offline Data in Realistic Robotic Tasks</papertitle></a><br>
              <strong>Rafael Rafailov</strong>,
              Varun Kumar,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>,
              <a href="https://www.avisingh.org/">Avi Singh</a>,
              Mariano Phielipp,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Offline Reinforcement Learning Workshop - NeurIPS 2021</em><br>
                <a href="">arXiv</a>
                </p><p></p>
                <p>
                In this work we develop an offline model-based meta-RL algorithm that operates from images in tasks with sparse rewards. Our approach has three main components: a novel strategy to construct meta-exploration trajectories from offline data, which allows agents to learn meaningful meta-test time task inference strategy; representation learning via variational filtering and latent conservative model-free policy optimization. We show that our method completely solves a realistic meta-learning task involving robot manipulation, while naive combinations of previous approaches fail.
                </p>
                </td>
              </tr>
              

    <tr onmouseout="eborl_stop()" onmouseover="eborl_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="eborl_still" style="opacity: 0;"><img src="./files/CEBORL.png" width=150></div>
                  <img src="./files/CEBORL.png" width=150 >
              </div>
              <script type="text/javascript">
              function combo_start() {
                document.getElementById('eborl_still').style.opacity = "1";
              }
              function combo_stop() {
                document.getElementById('eborl_still').style.opacity = "0";
              }
              combo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://offline-rl-neurips.github.io/2021/pdf/53.pdf">
                  <papertitle>Example-Based Offline Reinforcement Learning without Rewards</papertitle></a><br>
              Kyle Hatch*,
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu*</a>,
              <strong>Rafael Rafailov</strong>, 
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Offline Reinforcement Learning Workshop - NeurIPS 2021, In Submission</em><br>
                <a href="">arXiv</a>
                </p><p></p>
                <p>
                Offline RL methods require human-defined reward labels to learn from offline datasets. Reward specification remains a major challenge for deep RL algorithms in the real world since designing reward functions could take considerable manual effort. In contrast, in many settings, it is easier for users to provide examples of a completed task such as images than specifying a complex reward function. Based on this observation, we propose an algorithm that can learn behaviors from offline datasets without reward labels, instead using a small number of example images.
                </p>
                </td>
              </tr>
             

    <tr onmouseout="lompo_stop()" onmouseover="lompo_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="lompo_still" style="opacity: 0;"><img src="./files/lompo.png" width=150></div>
                  <img src="./files/lompo.png" width=150 >
              </div>
              <script type="text/javascript">
              function lompo_start() {
                document.getElementById('lompo_still').style.opacity = "1";
              }
     function lompo_stop() {
                document.getElementById('lompo_still').style.opacity = "0";
              }
              lompo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2012.11547.pdf">
                  <papertitle>Offline Reinforcement Learning from Images with Latent Space Models</papertitle></a><br>
                <strong>Rafael Rafailov*</strong>, 
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu*</a>,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Learning for Decision Making and Control (L4DC), 2021</em><font color="red"><strong> (Oral)</strong></font><br>
                <a href="https://arxiv.org/abs/2012.11547">arXiv</a> / <a href="https://sites.google.com/view/lompo/">website</a> / <a href="https://github.com/rmrafailov/LOMPO/">code</a>
                </p><p></p>
                <p>
                Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. We find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods in both simulated and real-world robotics control tasks.
                </p>
                </td>
              </tr>

    

    <tr onmouseout="macaw_stop()" onmouseover="macaw_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="macaw_still" style="opacity: 0;"><img src="./files/macaw.png" width=150></div>
                  <img src="./files/macaw.png" width=150 >
              </div>
              <script type="text/javascript">
              function combo_start() {
                document.getElementById('macaw_still').style.opacity = "1";
              }
     function combo_stop() {
                document.getElementById('macaw_still').style.opacity = "0";
              }
              combo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/abs/2008.06043">
                  <papertitle>Offline Meta-Reinforcement Learning with Advantage Weighting</papertitle></a><br>
              <a href="https://ericmitchell.ai/">Eric Mitchell</a>,
              <strong>Rafael Rafailov</strong>, 
              <a href="https://xbpeng.github.io/">Xue Bin Peng</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>International Conference on Machine Learning (ICML), 2021</em><br>
                <a href="href="https://arxiv.org/abs/2008.06043">arXiv</a> / <a href="hhttps://github.com/eric-mitchell/macaw">code</a>
                </p><p></p>
                <p>
                Offline meta-RL is analogous to the widely successful supervised learning strategy of pre-training a model on a large batch of fixed, pre-collected data (possibly from various tasks) and fine-tuning the model to a new task with relatively little data. By nature of being offline, algorithms for offline meta-RL can utilize the largest possible pool of training data available and eliminate potentially unsafe or costly data collection during meta-training. This setting inherits the challenges of offline RL, but it differs significantly because offline RL does not generally consider a) transfer to new tasks or b) limited data from the test task, both of which we face in offline meta-RL. Targeting the offline meta-RL setting, we propose Meta-Actor Critic with Advantage Weighting (MACAW), an optimization-based meta-learning algorithm that uses simple, supervised regression objectives for both the inner and outer loop of meta-training.
                </p>
                </td>
              </tr>
		


	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Teaching</heading>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <!--<td width="25%"><img src="./files/teach_crop.jpg" alt="teach" width="160" height="160"></td>-->
        <td width="75%" valign="center">
        <p>
        <a href="http://cs330.stanford.edu/">
            <papertitle>CS330: Deep Multi-Task and Meta Learning - Fall 2021</papertitle>
        </a>
        <br>
        Head Course Assistant
        </p>
        <p>
        <a href="http://cs330.stanford.edu/">
            <papertitle>CS330: Deep Multi-Task and Meta Learning - Fall 2020</papertitle>
        </a>
        <br>
        Course Assistant
        </p>
        <p>
        <a href="https://web.stanford.edu/class/cs379c/">
            <papertitle>CS379C: Computational Models of the Neocortex - Spring 2020</papertitle>
        </a>
        <br>
        Course Assistant
        </p>
        <p>
        </td>
      </tr>
      </tbody></table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td>
		<br>
		<p align="right"><font size="2">
		  <a href="http://www.cs.berkeley.edu/~barron/">Template</a>
		  </font>
		</p>
		</td>
	  </tr>
	  </table>
	</td>
	</tr>
  </table>
  </body>
</html>

