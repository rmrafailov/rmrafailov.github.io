
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-110963980-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-110963980-1');
	</script>

  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
	/* Color scheme stolen from Sergey Karayev */
	a {
	color: #1772d0;
	text-decoration:none;
	}
	a:focus, a:hover {
	color: #f09228;
	text-decoration:none;
	}
	body,td,th,tr,p,a {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px
	}
	strong {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px;
	}
	heading {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 22px;
	}
	papertitle {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 14px;
	font-weight: 700
	}
	name {
	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	font-size: 32px;
	}
	.one
	{
	width: 160px;
	height: 100px;
	position: relative;
	}
	.two
	{
	width: 160px;
	height: 100px;
	position: absolute;
	transition: opacity .2s ease-in-out;
	-moz-transition: opacity .2s ease-in-out;
	-webkit-transition: opacity .2s ease-in-out;
	}
	.fade {
	 transition: opacity .2s ease-in-out;
	 -moz-transition: opacity .2s ease-in-out;
	 -webkit-transition: opacity .2s ease-in-out;
	}
	span.highlight {
		background-color: #ffffd0;
	}
  </style>
  <link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~barron/seal_icon.png">
  <title>Tianhe Yu</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
	<tr>
	<td>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td width="67%" valign="middle">
		<p align="center">
		<name>Tianhe (Kevin) Yu</name><br>
		tianheyu at cs dot stanford dot edu
		</p>
		<p>
	  I am a PhD candidate in CS at <a href="https://www.stanford.edu/">Stanford University</a> advised by <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>. I am a part of <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a>.
	</p>
	<p>
	  Previously, I graduated from <a href="http://www.berkeley.edu/">UC Berkeley</a> with highest honors in
	  <a href="http://eecs.berkeley.edu/">Computer Science</a>, <a href="http://math.berkeley.edu/">Applied Mathematics</a> and <a href="http://statistics.berkeley.edu//">Statistics</a>. During my undergraduate study, I worked with <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, and <a href="https://people.eecs.berkeley.edu/~efros/">Alexei Efros</a> as an undergraduate researcher in the <a href="http://bair.berkeley.edu/">Berkeley Artificial Intelligence Research (BAIR)</a> Lab.
		</p>
		<p align=center>
<!-- <a href="https://tianheyu927.github.io/files/CV.pdf">CV</a> &nbsp/&nbsp -->
<a href="https://scholar.google.com/citations?user=5VaXUQsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
<a href="https://www.linkedin.com/in/tianhe-yu-791484b2/">LinkedIn</a> &nbsp/&nbsp
<!-- <a href="https://scholar.google.com/citations?user=Ivot3fkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp  -->
<a href="http://github.com/tianheyu927/"> GitHub </a>
		</p>
		</td>
		<td width="40%">
		<img src="./files/kevin.png" width="250">
		</td>
	  </tr>
	  </table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
            <heading>News</heading>
            <ul>
              <li><strong>September 2019</strong>: We released <a href="https://meta-world.github.io/">Meta-World</a> for benchmarking multi-task and meta-reinforcement learning algorithms!</li>
              <li><strong>December 2017</strong>: I was selected as the <strong>Honorable Mention</strong> for the <a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/">2018 Computing Research Association's (CRA) Outstanding Undergraduate Researcher Award</a>.</li>
              <li><strong>December 2017</strong>: At <a href="html://nips.cc">NIPS 2017</a>, I presented our work <a href="https://arxiv.org/pdf/1709.04905.pdf">"One-Shot Visual Imitation Learning via Meta-Learning"</a> at the <a href="https://sites.google.com/view/deeprl-symposium-nips2017/">Deep Reinforcement Learning Symposium</a> (<a href="https://vimeo.com/252186304">video here</a>).</li>
              <li><strong>December 2017</strong>: At <a href="html://nips.cc">NIPS 2017</a>, we did a live robot demonstration of meta-imitation learning and visual foresight during the <a href="https://nips.cc/Conferences/2017/Schedule?showEvent=9756">Tuesday Session</a>. The demo website is <a href="http://rail.eecs.berkeley.edu/nips_demo.html">here</a>.</li>
            </ul>
        </td></tr>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td width="100%" valign="middle">
		  <heading>Research</heading>
		  <p>
		My research interests lie at the intersection of machine learning, perception, and control for robotics, specifically deep reinforcement learning, imitation learning and meta-learning.
		  </p>
		</td>
	  </tr>
	  </table>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="cds_stop()" onmouseover="cds_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="cds_still" style="opacity: 0;"><img src="./files/cds.png" width=150></div>
                  <img src="./files/cds.png" width=150 >
              </div>
              <script type="text/javascript">
              function cds_start() {
                document.getElementById('cds_still').style.opacity = "1";
              }
     function cds_stop() {
                document.getElementById('cds_still').style.opacity = "0";
              }
              cds_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2109.08128.pdf">
                  <papertitle>Conservative Data Sharing for Multi-Task Offline Reinforcement Learning</papertitle></a><br>
                <strong>Tianhe Yu*</strong>,
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar*</a>,
              <a href="https://scholar.google.com/citations?user=ADkiClQAAAAJ&hl=en">Yevgen Chebotar</a>,
              <a href="https://karolhausman.github.io/">Karol Hausman</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em><br>
                <a href="https://arxiv.org/abs/2109.08128">arXiv</a>
                </p><p></p>
                <p>
                We argue that a natural use case of offline RL is in settings where we can pool large amounts of data collected in various scenarios for solving different tasks. However, sharing data across all tasks in multi-task offline RL performs surprisingly poorly in practice due to exacerbation of the distributional shift. To address this challenge, we develop a simple technique for data-sharing in multi-task offline RL that routes data based on the improvement over the task-specific data.
                </p>
                </td>
              </tr>

    <tr onmouseout="combo_stop()" onmouseover="combo_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="combo_still" style="opacity: 0;"><img src="./files/combo.png" width=150></div>
                  <img src="./files/combo.png" width=150 >
              </div>
              <script type="text/javascript">
              function combo_start() {
                document.getElementById('combo_still').style.opacity = "1";
              }
     function combo_stop() {
                document.getElementById('combo_still').style.opacity = "0";
              }
              combo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2102.08363.pdf">
                  <papertitle>COMBO: Conservative Offline Model-Based Policy Optimization</papertitle></a><br>
                <strong>Tianhe Yu*</strong>,
              <a href="https://aviralkumar2907.github.io/">Aviral Kumar*</a>,
              Rafael Rafailov, 
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em><br>
                <a href="https://arxiv.org/abs/2102.08363">arXiv</a>
                </p><p></p>
                <p>
                Model-based offline RL methods rely on explicit uncertainty quantification for incorporating pessimism, which can be difficult and unreliable with complex models. We overcome this limitation by developing a new model-based offline RL algorithm, COMBO, that regularizes the value function on out-of-support state-action tuples generated via rollouts under the learned model. We show COMBO with theoretical guarantees and also find that COMBO consistently performs as well or better as compared to prior offline model-free and model-based methods on widely studied offline RL benchmarks, including image-based tasks.
                </p>
                </td>
              </tr>

    <tr onmouseout="vmail_stop()" onmouseover="vmail_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="vmail_still" style="opacity: 0;"><img src="./files/vmail.png" width=150></div>
                  <img src="./files/vmail.png" width=150 >
              </div>
              <script type="text/javascript">
              function vmail_start() {
                document.getElementById('vmail_still').style.opacity = "1";
              }
     function vmail_stop() {
                document.getElementById('vmail_still').style.opacity = "0";
              }
              vmail_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2107.08829.pdf">
                  <papertitle>Visual Adversarial Imitation Learning using Variational Models</papertitle></a><br>
              Rafael Rafailov,
                <strong>Tianhe Yu</strong>,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em><br>
                <a href="https://arxiv.org/abs/2107.08829">arXiv</a>
                </p><p></p>
                <p>
                We develop a variational model-based adversarial imitation learning (V-MAIL) algorithm. The model-based approach provides a strong signal for representation learning, enables sample efficiency, and improves the stability of adversarial training. 
                </p>
                </td>
              </tr>

    <tr onmouseout="tag_stop()" onmouseover="tag_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="tag_still" style="opacity: 0;"><img src="./files/tag.png" width=150></div>
                  <img src="./files/tag.png" width=150 >
              </div>
              <script type="text/javascript">
              function tag_start() {
                document.getElementById('tag_still').style.opacity = "1";
              }
     function tag_stop() {
                document.getElementById('tag_still').style.opacity = "0";
              }
              tag_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2109.04617.pdf">
                  <papertitle>Efficiently Identifying Task Groupings for Multi-Task Learning</papertitle></a><br>
              <a href="https://www.linkedin.com/in/christopher-fifty/">Christopher Fifty</a>,
              <a href="https://scholar.google.fi/citations?user=F6omR3gAAAAJ&hl=en">Ehsan Amid</a>, 
              <a href="http://www-personal.umich.edu/~zhezhao/">Zhe Zhao</a>,
              <strong>Tianhe Yu</strong>,
              <a href="https://scholar.google.com/citations?user=Mv71-IcAAAAJ/">Rohan Anil</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2021</em></em><font color="red"><strong> (Spotlight)</strong></font><br>
                <a href="https://arxiv.org/abs/2109.04617">arXiv</a> / <a href="https://github.com/google-research/google-research/tree/master/tag">code</a>
                </p><p></p>
                <p>
                We find that in multi-task learning, na&#239;vely training all tasks together in one model often degrades performance, and exhaustively searching through combinations of task groupings can be prohibitively expensive. In this paper, we suggest an approach to select which tasks should train together in multi-task learning models. Our method determines task groupings in a single training run by co-training all tasks together and quantifying the effect to which one task's gradient would affect another task's loss. 
                </p>
                </td>
              </tr>

    <tr onmouseout="lompo_stop()" onmouseover="lompo_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="lompo_still" style="opacity: 0;"><img src="./files/lompo.png" width=150></div>
                  <img src="./files/lompo.png" width=150 >
              </div>
              <script type="text/javascript">
              function lompo_start() {
                document.getElementById('lompo_still').style.opacity = "1";
              }
     function lompo_stop() {
                document.getElementById('lompo_still').style.opacity = "0";
              }
              lompo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2012.11547.pdf">
                  <papertitle>Offline Reinforcement Learning from Images with Latent Space Models</papertitle></a><br>
                Rafael Rafailov*, 
                <strong>Tianhe Yu*</strong>,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Learning for Decision Making and Control (L4DC), 2021</em><font color="red"><strong> (Oral presentation)</strong></font><br>
                <a href="https://arxiv.org/abs/2012.11547">arXiv</a> / <a href="https://sites.google.com/view/lompo/">website</a>
                </p><p></p>
                <p>
                Model-based offline RL algorithms have achieved state of the art results in state based tasks and have strong theoretical guarantees. However, they rely crucially on the ability to quantify uncertainty in the model predictions, which is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. We find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods in both simulated and real-world robotics control tasks.
                </p>
                </td>
              </tr>

    <tr onmouseout="vsmaml_stop()" onmouseover="vsmaml_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="vsmaml_still" style="opacity: 0;"><img src="./files/vsmaml.png" width=150></div>
                  <img src="./files/vsmaml.png" width=150 >
              </div>
              <script type="text/javascript">
              function vsmaml_start() {
                document.getElementById('vsmaml_still').style.opacity = "1";
              }
     function vsmaml_stop() {
                document.getElementById('vsmaml_still').style.opacity = "0";
              }
              vsmaml_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2012.07769.pdf">
                  <papertitle>Variable-Shot Adaptation for Online Meta-Learning</papertitle></a><br>
                <strong>Tianhe Yu*</strong>,
              <a href="http://young-geng.xyz/">Xinyang Geng*</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
                <em>arXiv preprint</em><br>
                <a href="https://arxiv.org/abs/2012.07769">arXiv</a>
                </p><p></p>
                <p>
                We extend previous meta-learning algorithms to handle the variable-shot settings that naturally arise in sequential learning: from many-shot learning at the start, to zero-shot learning towards the end. On sequential learning problems, we find that meta-learning solves the full task set with fewer overall labels and achieves greater cumulative performance, compared to standard supervised methods. These results suggest that meta-learning is an important ingredient for building learning systems that continuously learn and improve over a sequence of problems.
                </p>
                </td>
              </tr>

    <tr onmouseout="mopo_stop()" onmouseover="mopo_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="mopo_still" style="opacity: 0;"><img src="./files/mopo.png" width=150></div>
                  <img src="./files/mopo.png" width=150 >
              </div>
              <script type="text/javascript">
              function mopo_start() {
                document.getElementById('mopo_still').style.opacity = "1";
              }
     function mopo_stop() {
                document.getElementById('mopo_still').style.opacity = "0";
              }
              mopo_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2005.13239.pdf">
                  <papertitle>MOPO: Model-based Offline Policy Optimization</papertitle></a><br>
                <strong>Tianhe Yu*</strong>,
              <a href="https://ai.stanford.edu/~gwthomas/">Garrett Thomas*</a>,
              <a href="http://lantaoyu.com/">Lantao Yu</a>, 
              <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>, 
              <a href="https://www.james-zou.com/">James Zou</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn&dagger;</a>,
              <a href="http://ai.stanford.edu/~tengyuma/">Tengyu Ma&dagger;</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2020</em><br>
                <a href="https://arxiv.org/abs/2005.13239">arXiv</a> / <a href="https://github.com/tianheyu927/mopo">code</a>
                </p><p></p>
                <p>
                We propose a new model-based offline RL algorithm that applies the uncertainty of the dynamics as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as challenging continuous control tasks that require generalizing from data collected for a different task.
                </p>
                </td>
              </tr>

	  <tr onmouseout="pcgrad_stop()" onmouseover="pcgrad_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="pcgrad_still" style="opacity: 0;"><img src="./files/pcgrad.png" width=150></div>
                  <img src="./files/pcgrad.png" width=150 >
              </div>
              <script type="text/javascript">
              function pcgrad_start() {
                document.getElementById('pcgrad_still').style.opacity = "1";
              }
     function pcgrad_stop() {
                document.getElementById('pcgrad_still').style.opacity = "0";
              }
              pcgrad_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/2001.06782.pdf">
                  <papertitle>Gradient Surgery for Multi-Task Learning</papertitle></a><br>
                <strong>Tianhe Yu</strong>,
	            <a href="https://scholar.google.com/citations?user=Rkr2uT8AAAAJ&hl=en">Saurabh Kumar</a>,
	            <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
	            <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
	            <a href="https://karolhausman.github.io/">Karol Hausman</a>, 
	            <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2020</em><br>
                <a href="https://arxiv.org/abs/2001.06782">arXiv</a> / <a href="https://github.com/tianheyu927/PCGrad">code</a>
                </p><p></p>
                <p>
                We identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach, projecting conflicting gradients (PCGrad), for avoiding such interference between task gradients.
                </p>
                </td>
              </tr>

	  <tr onmouseout="metaworld_stop()" onmouseover="metaworld_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="metaworld_still" style="opacity: 0;"><img src="./files/metaworld.png" width=150></div>
                  <img src="./files/metaworld.png" width=150 >
              </div>
              <script type="text/javascript">
              function metaworld_start() {
                document.getElementById('metaworld_still').style.opacity = "1";
              }
     function metaworld_stop() {
                document.getElementById('metaworld_still').style.opacity = "0";
              }
              metaworld_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/1910.10897.pdf">
                  <papertitle>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</papertitle></a><br>
                <strong>Tianhe Yu*</strong>,
	            <a href="https://scholar.google.com/citations?user=eDQsOFMAAAAJ&hl=en/">Deirdre Quillen*</a>,
	            <a href="https://zhanpenghe.github.io">Zhanpeng He*</a>, 
	            <a href="https://ryanjulian.me/">Ryan Julian</a>, 
	            <a href="https://karolhausman.github.io/">Karol Hausman</a>, 
	            <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
	            <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
                <em>Conference on Robot Learning (CoRL)</em>, 2019<br>
                <a href="https://arxiv.org/abs/1910.10897">arXiv</a>  / <a href="https://meta-world.github.io/">website</a> / <a href="https://github.com/rlworkgroup/metaworld">code</a>
                </p><p></p>
                <p>
                We propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks, with the aim of making it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks.
                </p>
                </td>
              </tr>

	  <tr onmouseout="pemirl_stop()" onmouseover="pemirl_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="pemirl_still" style="opacity: 0;"><img src="./files/pemirl.png" width=150></div>
                  <img src="./files/pemirl.png" width=150 >
              </div>
              <script type="text/javascript">
              function pemirl_start() {
                document.getElementById('pemirl_still').style.opacity = "1";
              }
     function pemirl_stop() {
                document.getElementById('pemirl_still').style.opacity = "0";
              }
              pemirl_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/1909.09314.pdf">
                  <papertitle>Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</papertitle></a><br>
                <a href="http://lantaoyu.com/">Lantao Yu*</a>,
                <strong>Tianhe Yu*</strong>,
                <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
                <a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a><br>
                <em>Neural Information Processing Systems (NeurIPS), 2019</em><br>
                <a href="https://arxiv.org/abs/1909.09314">arXiv</a>  / <a href="https://sites.google.com/view/pemirl">website</a>
                </p><p></p>
                <p>
                We propose a deep latent variable model that is capable of learning rewards from unstructured, multi-task demonstration data, and critically, use this experience to infer robust rewards for new, structurally-similar tasks from a single demonstration.
                </p>
                </td>
              </tr>

      <tr onmouseout="hil_stop()" onmouseover="hil_start()">
            <td width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="hil_still" style="opacity: 0;"><img src="./files/hil.png" width=150></div>
                  <img src="./files/hil.png" width=150 >
              </div>
              <script type="text/javascript">
              function hil_start() {
                document.getElementById('hil_still').style.opacity = "1";
              }
     function hil_stop() {
                document.getElementById('hil_still').style.opacity = "0";
              }
              hil_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/1810.11043.pdf">
                  <papertitle>One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</papertitle></a><br>
                <strong>Tianhe Yu</strong>,
                <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
                <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
                <a href="http://people.eecs.berkeley.edu/~cbfinn">Chelsea Finn</a><br>
                <em>International Conference on Intelligent Robots and Systems (IROS), 2019</em><br>
                <a href="https://arxiv.org/abs/1810.11043">arXiv</a>  / <a href="https://sites.google.com/view/one-shot-hil">video</a>
                </p><p></p>
                <p>
                We aim to learn multi-stage vision-based tasks on a real robot from a single video of a human performing the task. We propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by "watching" a human demonstrator.
                </p>
                </td>
              </tr>

        <tr onmouseout="dpn_stop()" onmouseover="dpn_start()">
            <td valign="top" width="25%">
                  <heading2><i></i></heading2><br>
              <div class="one">
                  <div class="two" id="dpn_still" style="opacity: 0;"><img src="./files/dpn.png" width=150></div>
                  <img src="./files/dpn.png" width=150 >
              </div>
              <script type="text/javascript">
              function dpn_start() {
                document.getElementById('dpn_still').style.opacity = "1";
              }
     function dpn_stop() {
                document.getElementById('dpn_still').style.opacity = "0";
              }
              dpn_stop()
              </script>

                </td>
                <td valign="top" width="75%">
                <p><a href="https://arxiv.org/pdf/1902.05542.pdf">
                  <papertitle>Unsupervised Visuomotor Control through Distributional Planning Networks</papertitle></a><br>
                <strong>Tianhe Yu</strong>,
                Gleb Shevchuk,
                <a href="https://dorsa.fyi/">Dorsa Sadigh</a>,
                <a href="http://people.eecs.berkeley.edu/~cbfinn">Chelsea Finn</a><br>
                <em>Robotics: Science and Systems (RSS), 2019</em><br>
                <a href="https://arxiv.org/abs/1902.05542">arXiv</a>  / <a href="https://sites.google.com/view/dpn-public/">website</a> / <a href="https://github.com/tianheyu927/dpn">code</a>
                </p><p></p>
                <p>
                We propose an approach to learning an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our method enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms.
                </p>
                </td>
              </tr>

		<tr onmouseout="pick_place_stop()" onmouseover="pick_place_start()">
		  <td width="25%">
			<div class="one">
				<div class="two" id="pick_place_gif" style="opacity: 0;"><img src="./files/human_imitation.gif" width=150 height=120></div>
				<img src="./files/human_imitation.png" width=150 height=120>
			</div>
			<script type="text/javascript">
			function pick_place_start() {
			  document.getElementById('pick_place_gif').style.opacity = "1";
			}
   			function pick_place_stop() {
			  document.getElementById('pick_place_gif').style.opacity = "0";
			}
			place_stop()
			</script>

			  </td>
			  <td valign="top" width="75%">
				<heading2><i></i></heading2><br>
			  <p><a href="https://arxiv.org/pdf/1802.01557.pdf">
				<papertitle>One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</papertitle></a><br>
			  <strong>Tianhe Yu*</strong>,
			  <a href="http://people.eecs.berkeley.edu/~cbfinn">Chelsea Finn</a>*,
			   Annie Xie, Sudeep Dasari,
			  <a href="http://tianhaozhang.com/">Tianhao Zhang</a>,
			  <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
			  <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
			  <em>Robotics: Science and Systems (RSS)</em>, 2018<br>
				<a href="https://arxiv.org/abs/1802.01557">arXiv</a>
				/
				<a href="http://bair.berkeley.edu/blog/2018/06/28/daml/">blog post</a>
				/
				<a href="https://sites.google.com/view/daml">video</a>
				/
				<a href="https://github.com/tianheyu927/mil">code</a>
			  </p><p></p>
			  <p> We present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated.
			  </p>
			  </td>
			</tr>
	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr onmouseout="place_stop()" onmouseover="place_start()">
		  <td width="25%">
			<div class="one">
				<div class="two" id="place_gif" style="opacity: 0;"><img src="./files/imitationsmall.gif" width=150 height=150></div>
				<img src="./files/imitation_image.png" width=150 height=150>
			</div>
			<script type="text/javascript">
			function place_start() {
			  document.getElementById('place_gif').style.opacity = "1";
			}
   			function place_stop() {
			  document.getElementById('place_gif').style.opacity = "0";
			}
			place_stop()
			</script>

			  </td>
			  <td valign="top" width="75%">
				<heading2><i></i></heading2><br>
			  <p><a href="https://arxiv.org/pdf/1709.04905.pdf">
				<papertitle>One-Shot Visual Imitation Learning via Meta-Learning</papertitle></a><br>
			  <a href="http://people.eecs.berkeley.edu/~cbfinn">Chelsea Finn</a>*,
			  <strong>Tianhe Yu*</strong>,
			  <a href="http://tianhaozhang.com/">Tianhao Zhang</a>,
			  <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
			  <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
			  <em>Conference on Robot Learning (CoRL)</em>, 2017 <font color="red"><strong>(Long Talk)</strong></font> <br>
			  <strong style="color:green">Oral presentation at the <a href="https://sites.google.com/view/deeprl-symposium-nips2017/">NIPS 2017 Deep Reinforcement Learning Symposium</a></strong><br>
				<a href="https://arxiv.org/abs/1709.04905">arXiv</a>
				/
				<a href="https://sites.google.com/view/one-shot-imitation">video</a>
				/
				<a href="https://youtu.be/_9Ny2ghjwuY?t=7058">talk</a>
				/
				<a href="https://github.com/tianheyu927/mil">code</a>
			  </p><p></p>
			  <p> We present a meta-imitation learning method that enables a robot to learn to acquire new skills from just a single visual demonstration. Our method requires data from significantly fewer prior tasks for effective learning of new skills and can also learns from a raw video as the single demonstration without access to trajectories of robot configurations such as joint angles.
			  </p>
			  </td>
			</tr>

		<tr>
		  <td width="25%">

				  <heading2><i></i></heading2><br>
			<div class="one">
				<div class="two" id="icolor_image" style="opacity: 0;"><img src="./files/icolor_wide.png" width=150 height=100></div>
				<img src="./files/icolor_wide.png" width=150 height=100>
			</div>

			  </td>
			  <td valign="top" width="75%">
			  <p><a href="https://arxiv.org/pdf/1705.02999.pdf">
				<papertitle>Real-Time User-Guided Image Colorization with Learned Deep Priors</papertitle></a><br>
			  <a href="http://richzhang.github.io/">Richard Zhang</a>*, <a href="https://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>*, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://young-geng.xyz/">Xinyang Geng</a>, Angela S. Lin, <strong>Tianhe Yu</strong>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a> <br>
			  <em>ACM Transactions on Graphics (SIGGRAPH)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
				<a href="https://arxiv.org/abs/1705.02999">arXiv</a>
				/
				<a href="https://richzhang.github.io/ideepcolor/">project website</a>
				/
				<a href="https://www.youtube.com/watch?v=eL5ilZgM89Q&feature=youtu.be">video</a>
				/
				<a href="https://www.dropbox.com/s/urmifx558nw0ogi/release.pptx?dl=0">slides</a>
				/
				<a href="https://www.youtube.com/watch?v=FTzcFsz2xqw&feature=youtu.be&t=992">talk<a>
				/
				<a href="https://github.com/junyanz/interactive-deep-colorization">code</a>
			  </p><p></p>
			  <p> We propose a deep learning approach for user-guided image colorization. Our system directly maps a grayscale image, along with sparse, local user "hints" to an output colorization with a deep convolutional neural network.
			  </p>
			  </td>
			</tr>

		<tr onmouseout="ssrl_stop()" onmouseover="ssrl_start()">
		  <td width="25%">

				  <heading2><i></i></heading2><br>
			<div class="one">
				<div class="two" id="ssrl_image" style="opacity: 0;"><img src="./files/ssrl_crop.png" width=150 height=130></div>
				<img src="./files/ssrl_crop.png" width=150 height=130>
			</div>
			<script type="text/javascript">
			function ssrl_start() {
			  document.getElementById('ssrl_image').style.opacity = "1";
			}
			function ssrl_stop() {
			  document.getElementById('ssrl_image').style.opacity = "0";
			}
			ssrl_stop()
			</script>

			  </td>
			  <td valign="top" width="75%">
			  <p><a href="https://arxiv.org/pdf/1612.00429.pdf">
				<papertitle>Generalizing Skills with Semi-Supervised Reinforcement Learning</papertitle></a><br>
			  <a href="http://people.eecs.berkeley.edu/~cbfinn">Chelsea Finn</a>, <strong>Tianhe Yu</strong>, <a href="http://people.eecs.berkeley.edu/~justinjfu">Justin Fu</a>,
			  <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
			  <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
			  <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
				<a href="https://arxiv.org/abs/1612.00429">arXiv</a>
				/
				<a href="https://sites.google.com/site/semisupervisedrl">video</a>
				/
				<a href="https://github.com/cbfinn/gps/tree/ssrl">code</a>
			  </p><p></p>
			  <p> We formalize the problem of semi-supervised reinforcement learning (SSRL), where the reward signal in the real world is only available in a small set of environments such as laboratories, and the robot need to leverage experiences in these instrument environments to continue learning in places where reward signal isn't available.
			  We propose a simple algorithm for SSRL based on inverse reinforcement learning and show that it can improve performance in 'unlabeled' environments by using experience from both 'labeled' environments and 'unlabeled' environments.
			  </p>
			  </td>
			</tr>


	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>Teaching</heading>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <!--<td width="25%"><img src="./files/teach_crop.jpg" alt="teach" width="160" height="160"></td>-->
        <td width="75%" valign="center">
          <p>
        <a href="http://cs330.stanford.edu/">
            <papertitle>CS330: Deep Multi-Task and Meta Learning - Fall 2019</papertitle>
        </a>
        <br>
        Teaching Assistant

        </p>
        </td>
      </tr>
      </tbody></table>

	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	  <tr>
		<td>
		<br>
		<p align="right"><font size="2">
		  <a href="http://www.cs.berkeley.edu/~barron/">Template</a>
		  </font>
		</p>
		</td>
	  </tr>
	  </table>
	</td>
	</tr>
  </table>
  </body>
</html>

